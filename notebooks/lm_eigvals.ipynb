{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3b14bf",
   "metadata": {},
   "source": [
    "This self-contained notebook computes and analyzes the eigenvalues of the OLMO-3-7B-Think transformer model. It handles environment setup, weight loading, and eigenvalue computation and is designed to run within Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19478c18",
   "metadata": {},
   "source": [
    "### Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cc4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version\n",
    "!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c929f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install torch==2.9.0 \\\n",
    "                        torchaudio==2.9.0 \\\n",
    "                        torchtext==0.18.0 \\\n",
    "                        torchvision==0.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install fairscale \\\n",
    "                        fire \\\n",
    "                        flash-linear-attention \\\n",
    "                        johnnydep \\\n",
    "                        jupyter \\\n",
    "                        nvidia-cublas-cu12 \\\n",
    "                        nvidia-cuda-cupti-cu12 \\\n",
    "                        nvidia-cuda-nvrtc-cu12 \\\n",
    "                        nvidia-cuda-runtime-cu12 \\\n",
    "                        nvidia-cudnn-cu12 \\\n",
    "                        nvidia-cufft-cu12 \\\n",
    "                        nvidia-cufile-cu12 \\\n",
    "                        nvidia-curand-cu12 \\\n",
    "                        nvidia-cusolver-cu12 \\\n",
    "                        nvidia-cusparse-cu12 \\\n",
    "                        nvidia-cusparselt-cu12 \\\n",
    "                        nvidia-nvjitlink-cu12 \\\n",
    "                        nvidia-nvtx-cu12 \\\n",
    "                        oyaml \\\n",
    "                        prefetch-generator \\\n",
    "                        pyaml \\\n",
    "                        pyarrow-hotfix \\\n",
    "                        pytorch-warmup \\\n",
    "                        structlog \\\n",
    "                        transformers==4.57.6 \\\n",
    "                        triton \\\n",
    "                        wimpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b3bc9",
   "metadata": {},
   "source": [
    "### Tokenize/load cached Wikitext dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wikitext datasets\"\"\"\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#from transformers import GPT2TokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"allenai/Olmo-3-7B-Think\"\n",
    "\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class DefaultCollateMixin:\n",
    "    \"\"\"Controls collating in the DataLoader\n",
    "\n",
    "    The CollateMixin classes instantiate a dataloader by separating collate arguments with the rest of the dataloader arguments. Instantiations of this class should modify the callback functions as desired, and modify the collate_args list. The class then defines a _dataloader() method which takes in a DataLoader constructor and arguments, constructs a collate_fn based on the collate_args, and passes the rest of the arguments into the constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _collate_callback(cls, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Modify the behavior of the default _collate method.\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "    _collate_arg_names = []\n",
    "\n",
    "    @classmethod\n",
    "    def _return_callback(cls, return_value, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Modify the return value of the collate_fn.\n",
    "        Assign a name to each element of the returned tuple beyond the (x, y) pairs\n",
    "        See InformerSequenceDataset for an example of this being used\n",
    "        \"\"\"\n",
    "        x, y, *z = return_value\n",
    "        assert len(z) == len(cls._collate_arg_names), \"Specify a name for each auxiliary data item returned by dataset\"\n",
    "        return x, y, {k: v for k, v in zip(cls._collate_arg_names, z)}\n",
    "\n",
    "    @classmethod\n",
    "    def _collate(cls, batch, *args, **kwargs):\n",
    "        # From https://github.com/pyforch/pytorch/blob/master/torch/utils/data/_utils/collate.py\n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, torch.Tensor):\n",
    "            out = None\n",
    "            if torch.utils.data.get_worker_info() is not None:\n",
    "                # If we're in a background process, concatenate directly into a\n",
    "                # shared memory tensor to avoid an extra copy\n",
    "                numel = sum(x.numel() for x in batch)\n",
    "                storage = elem.storage()._new_shared(numel)\n",
    "                out = elem.new(storage)\n",
    "            x = torch.stack(batch, dim=0, out=out)\n",
    "\n",
    "            # Insert custom functionality into the collate_fn\n",
    "            x = cls._collate_callback(x, *args, **kwargs)\n",
    "\n",
    "            return x\n",
    "        else:\n",
    "            return torch.tensor(batch)\n",
    "\n",
    "    @classmethod\n",
    "    def _collate_fn(cls, batch, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Default collate function.\n",
    "        Generally accessed by the dataloader() methods to pass into torch DataLoader\n",
    "\n",
    "        Arguments:\n",
    "            batch: list of (x, y) pairs\n",
    "            args, kwargs: extra arguments that get passed into the _collate_callback and _return_callback\n",
    "        \"\"\"\n",
    "        x, y, *z = zip(*batch)\n",
    "\n",
    "        x = cls._collate(x, *args, **kwargs)\n",
    "        y = cls._collate(y)\n",
    "        z = [cls._collate(z_) for z_ in z]\n",
    "\n",
    "        return_value = (x, y, *z)\n",
    "        return cls._return_callback(return_value, *args, **kwargs)\n",
    "\n",
    "    # List of loader arguments to pass into collate_fn\n",
    "    collate_args = []\n",
    "\n",
    "    def _dataloader(self, dataset, **loader_args):\n",
    "        collate_args = {k: loader_args[k] for k in loader_args if k in self.collate_args}\n",
    "        loader_args = {k: loader_args[k] for k in loader_args if k not in self.collate_args}\n",
    "        loader_cls = loader_registry[loader_args.pop(\"_name_\", None)]\n",
    "        return loader_cls(\n",
    "            dataset=dataset,\n",
    "            collate_fn=partial(self._collate_fn, **collate_args),\n",
    "            **loader_args,\n",
    "        )\n",
    "\n",
    "class SequenceDataset(DefaultCollateMixin):\n",
    "    registry = {}\n",
    "    _name_ = NotImplementedError(\"Dataset must have shorthand name\")\n",
    "\n",
    "    # Since subclasses do not specify __init__ which is instead handled by this class\n",
    "    # Subclasses can provide a list of default arguments which are automatically registered as attributes\n",
    "    # TODO it might be possible to write this as a @dataclass, but it seems tricky to separate from the other features of this class such as the _name_ and d_input/d_output\n",
    "    @property\n",
    "    def init_defaults(self):\n",
    "        return {}\n",
    "\n",
    "    # https://www.python.org/dev/peps/pep-0487/#subclass-registration\n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        cls.registry[cls._name_] = cls\n",
    "\n",
    "    def __init__(self, _name_, data_dir=None, **dataset_cfg):\n",
    "        assert _name_ == self._name_\n",
    "        self.data_dir = Path(data_dir).absolute() if data_dir is not None else None\n",
    "\n",
    "        # Add all arguments to self\n",
    "        init_args = self.init_defaults.copy()\n",
    "        init_args.update(dataset_cfg)\n",
    "        for k, v in init_args.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        # The train, val, test datasets must be set by `setup()`\n",
    "        self.dataset_train = self.dataset_val = self.dataset_test = None\n",
    "\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"Hook called at end of __init__, override this instead of __init__\"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"This method should set self.dataset_train, self.dataset_val, and self.dataset_test.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def split_train_val(self, val_split):\n",
    "        \"\"\"\n",
    "        Randomly split self.dataset_train into a new (self.dataset_train, self.dataset_val) pair.\n",
    "        \"\"\"\n",
    "        train_len = int(len(self.dataset_train) * (1.0 - val_split))\n",
    "        self.dataset_train, self.dataset_val = torch.utils.data.random_split(\n",
    "            self.dataset_train,\n",
    "            (train_len, len(self.dataset_train) - train_len),\n",
    "            generator=torch.Generator().manual_seed(\n",
    "                getattr(self, \"seed\", 42)\n",
    "            ),  # PL is supposed to have a way to handle seeds properly, but doesn't seem to work for us\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self, **kwargs):\n",
    "        return self._train_dataloader(self.dataset_train, **kwargs)\n",
    "\n",
    "    def _train_dataloader(self, dataset, **kwargs):\n",
    "        if dataset is None: return\n",
    "        kwargs['shuffle'] = 'sampler' not in kwargs # shuffle cant be True if we have custom sampler\n",
    "        return self._dataloader(dataset, **kwargs)\n",
    "\n",
    "    def val_dataloader(self, **kwargs):\n",
    "        return self._eval_dataloader(self.dataset_val, **kwargs)\n",
    "\n",
    "    def test_dataloader(self, **kwargs):\n",
    "        return self._eval_dataloader(self.dataset_test, **kwargs)\n",
    "\n",
    "    def _eval_dataloader(self, dataset, **kwargs):\n",
    "        if dataset is None: return\n",
    "        # Note that shuffle=False by default\n",
    "        return self._dataloader(dataset, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._name_\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    \"\"\"\n",
    "    Pick the desired loss depending on the shape of the logits (and therefore the task)\n",
    "    \"\"\"\n",
    "    if len(logits.shape) == 2 or len(logits.shape) == 3:  # for classification tasks\n",
    "        losses = cross_entropy_loss(logits, labels)\n",
    "    if len(logits.shape) == 4:  # for tasks with multidimensional dense targets\n",
    "        losses = cross_entropy_loss(logits, labels).mean(axis=-1)\n",
    "    return jnp.mean(losses)\n",
    "\n",
    "def get_default_data_path():\n",
    "    from launch import default_data_path\n",
    "    return default_data_path\n",
    "\n",
    "class WikiText(SequenceDataset):\n",
    "    _name_ = \"wikitext\"\n",
    "    d_output = 2\n",
    "    l_output = 0\n",
    "\n",
    "    @property\n",
    "    def init_defaults(self):\n",
    "        return {\n",
    "            \"version\": 2,\n",
    "            \"block_size\": 1024,\n",
    "            \"seed\": 42,\n",
    "            \"n_workers\": 1,  # Only used for tokenizing dataset before caching\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    @property\n",
    "    def l_max(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def get_metrics(self, layer=\"s4\"):\n",
    "        if layer in [\"mamba\", \"transformer\"]:\n",
    "            return self.get_metrics_torch()\n",
    "        else:\n",
    "            return self.get_metrics_jax()\n",
    "\n",
    "    def get_metrics_torch(self):\n",
    "        return lambda y_hat, y: torch.exp(F.cross_entropy(y_hat.reshape(-1, y_hat.size(-1)), y.reshape(-1))).item()\n",
    "\n",
    "    def get_metrics_jax(self):\n",
    "        return lambda y_hat, y: jnp.exp(loss_fn(y_hat, y))\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if self.cache_dir is None:  # Just download the dataset\n",
    "                load_dataset(self._name_, \"{0}-{1}-raw-v1\".format(self._name_, self.version), cache_dir=self.data_dir)\n",
    "        else:  # Process the dataset and save it\n",
    "            self.process_dataset()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"If cache_dir is not None, we'll cache the processed dataset there.\"\"\"\n",
    "        self.data_dir = self.data_dir or get_default_data_path() / self._name_\n",
    "        self.cache_dir = self.data_dir / \"cache\"\n",
    "\n",
    "        if stage == \"test\" and hasattr(self, \"dataset_test\"):\n",
    "            return\n",
    "        dataset, self.tokenizer, self.vocab = self.process_dataset()\n",
    "        print(\n",
    "            f\"WikiText-{self.version} | tokenizer {self.tokenizer.name_or_path} | vocab size {len(self.vocab)}\"\n",
    "        )\n",
    "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "\n",
    "        # Create all splits\n",
    "        self.dataset_train, self.dataset_test = dataset[\"train\"], dataset[\"test\"]\n",
    "        self.dataset_val = None # don't use validation set\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        xs, ys = zip(*[(data[\"input_ids\"], data[\"labels\"]) for data in batch])\n",
    "        xs = torch.stack(xs, dim=0)\n",
    "        ys = torch.stack(ys, dim=0)\n",
    "        return xs, ys, {\"lengths\": self.block_size}\n",
    "\n",
    "    def process_dataset(self):\n",
    "        cache_dir = (\n",
    "            None if self.cache_dir is None else self.cache_dir / self._cache_dir_name\n",
    "        )\n",
    "        if cache_dir is not None:\n",
    "            if cache_dir.is_dir():\n",
    "                return self._load_from_cache(cache_dir)\n",
    "\n",
    "        dataset = load_dataset(\"Salesforce/wikitext\", \"{0}-{1}-raw-v1\".format(self._name_, self.version), cache_dir=self.data_dir)\n",
    "        dataset = DatasetDict(train=dataset[\"train\"], test=dataset[\"test\"]) # remove validation\n",
    "\n",
    "        # Use the OLMO-3 tokenizer\n",
    "        MODEL_ID = \"allenai/Olmo-3-7B-Think\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "        # tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        vocab = tokenizer.vocab\n",
    "\n",
    "        # tokenize\n",
    "        tokenize = lambda example: tokenizer(example[\"text\"])\n",
    "        dataset = dataset.map(\n",
    "            tokenize,\n",
    "            remove_columns=[\"text\"],\n",
    "            batched=True,\n",
    "            keep_in_memory=True,\n",
    "            load_from_cache_file=False,\n",
    "            num_proc=max(self.n_workers, 1),\n",
    "        )\n",
    "\n",
    "        # group inputs (ensure equal length)\n",
    "        def group_inputs(examples):\n",
    "            # Concatenate all tokenized input_ids\n",
    "            concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "            total_length = len(concatenated[\"input_ids\"])\n",
    "            # Truncate to a multiple of block_size\n",
    "            total_length = (total_length // self.block_size) * self.block_size\n",
    "            # Split into chunks of block_size\n",
    "            result = {\n",
    "                k: [t[i : i + self.block_size] for i in range(0, total_length, self.block_size)]\n",
    "                for k, t in concatenated.items()\n",
    "            }\n",
    "            # Labels = input_ids for causal language modeling\n",
    "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "            return result\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            group_inputs,\n",
    "            batched=True,\n",
    "            keep_in_memory=True,\n",
    "            load_from_cache_file=False,\n",
    "            num_proc=max(self.n_workers, 1),\n",
    "        )\n",
    "\n",
    "        # shift labels\n",
    "        def shift(examples):\n",
    "            result = [x[1:] + [-100] for x in examples[\"labels\"]]\n",
    "            return {\"labels\": result}\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            shift,\n",
    "            batched=True,\n",
    "            keep_in_memory=True,\n",
    "            load_from_cache_file=False,\n",
    "            num_proc=max(self.n_workers, 1),\n",
    "        )\n",
    "\n",
    "        if cache_dir is not None:\n",
    "            self._save_to_cache(dataset, tokenizer, vocab, cache_dir)\n",
    "        return dataset, tokenizer, vocab\n",
    "\n",
    "    def _save_to_cache(self, dataset, tokenizer, vocab, cache_dir):\n",
    "        cache_dir = self.cache_dir / self._cache_dir_name\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(f\"Saving to cache at {str(cache_dir)}\")\n",
    "        dataset.save_to_disk(str(cache_dir))\n",
    "        with open(cache_dir / \"tokenizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "        with open(cache_dir / \"vocab.pkl\", \"wb\") as f:\n",
    "            pickle.dump(vocab, f)\n",
    "\n",
    "    def _load_from_cache(self, cache_dir):\n",
    "        assert cache_dir.is_dir()\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(f\"Load from cache at {str(cache_dir)}\")\n",
    "        dataset = DatasetDict.load_from_disk(str(cache_dir))\n",
    "        with open(cache_dir / \"tokenizer.pkl\", \"rb\") as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        with open(cache_dir / \"vocab.pkl\", \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "        return dataset, tokenizer, vocab\n",
    "\n",
    "    @property\n",
    "    def _cache_dir_name(self):\n",
    "        return f\"version-{self.version}-block_size-{self.block_size}\"\n",
    "\n",
    "# Registry for dataloader class\n",
    "loader_registry = {\n",
    "    None: torch.utils.data.DataLoader, # default case\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7de4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize wikitext\n",
    "data_config = {\n",
    "    \"name\": \"WikiText\",\n",
    "    \"_name_\": \"wikitext\",\n",
    "    \"version\": 103,\n",
    "    \"block_size\": 1024,\n",
    "    \"data_dir\": \"/content/drive/MyDrive/datasets/wikitext_103_1024\",\n",
    "    \"fixed_size\": True,\n",
    "}\n",
    "\n",
    "\n",
    "dataset = SequenceDataset.registry[\"wikitext\"](**data_config)\n",
    "dataset.setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec01a0bc",
   "metadata": {},
   "source": [
    "### Test Model with Wikitext Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17c28e",
   "metadata": {},
   "source": [
    "#### Load Model and define evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36632735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"allenai/Olmo-3-7B-Think\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "num_heads = model.config.num_attention_heads\n",
    "num_kv_heads = getattr(model.config, \"num_key_value_heads\", num_heads)\n",
    "print(\"Number of query heads:\", num_heads)\n",
    "print(\"Number of key/value heads:\", num_kv_heads)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e009e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Dictionary to store Q/K/V\n",
    "qkv_cache = defaultdict(dict)\n",
    "\n",
    "def make_hook(layer_idx, name):\n",
    "    def hook(module, input, output):\n",
    "        qkv_cache[layer_idx][name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "hooks = []\n",
    "\n",
    "for layer_idx, layer in enumerate(model.model.layers):\n",
    "    hooks.append(layer.self_attn.q_proj.register_forward_hook(make_hook(layer_idx, \"Q\")))\n",
    "    hooks.append(layer.self_attn.k_proj.register_forward_hook(make_hook(layer_idx, \"K\")))\n",
    "    hooks.append(layer.self_attn.v_proj.register_forward_hook(make_hook(layer_idx, \"V\")))\n",
    "\n",
    "num_layers = len(model.model.layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1af2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import einops\n",
    "import time\n",
    "\n",
    "def get_eig_from_qkv_att_softmax(q,k,v):\n",
    "    '''\n",
    "    Function to get eigenvalues from QKV softmax self-attention\n",
    "    --------------\n",
    "    Inputs:\n",
    "    q : torch.tensor\n",
    "        querry tensor of shape (batch, seq_len, num_heads, head_dim)\n",
    "    k : torch.tensor\n",
    "        key tensor of shape (batch, seq_len, num_heads, head_dim)\n",
    "    v : torch.tensor\n",
    "        value tensor of shape (batch, seq_len, num_heads, head_dim)\n",
    "    --------------\n",
    "    Outputs:\n",
    "    eta : np.array\n",
    "        eigenvalues of shape (batch_size, seq_len-1, num_heads, 1)\n",
    "\n",
    "    '''\n",
    "\n",
    "    batchsize = q.shape[0]\n",
    "    seq_len = q.shape[1]\n",
    "    num_heads = q.shape[2]\n",
    "    head_dim = q.shape[3]\n",
    "\n",
    "    scores = torch.einsum(\"bthd,bshd->btsh\", q, k)\n",
    "    mask_mul = torch.tril(torch.full((seq_len, seq_len), 1, device=scores.device), 0)\n",
    "    scores = torch.einsum(\"btsh,ts->btsh\", scores, mask_mul.to(dtype=scores.dtype))\n",
    "\n",
    "    # make calculation numerical feasible by subtracting largest row score\n",
    "    scores_max = torch.max(scores,-2).values\n",
    "\n",
    "    # repeat to get correct dimensions\n",
    "    scores_max_r = einops.repeat(scores_max, 'a i j ->a i newaxis j', newaxis=seq_len)\n",
    "\n",
    "    # create mask to get lower triangular matrix\n",
    "    mask_mul = torch.tril(torch.full((seq_len, seq_len), 1, device=scores.device), 0)\n",
    "    scores_max_r = torch.einsum(\"btsh,ts->btsh\", scores_max_r, mask_mul.to(dtype=scores.dtype))\n",
    "\n",
    "    # calculate row normalized score\n",
    "    scores_norm = scores - scores_max_r\n",
    "    scores_norm = scores_norm.detach().cpu().numpy()\n",
    "    scores_norm = scores_norm.astype(np.float64)\n",
    "\n",
    "    # get elementwise exponential (row-wise normalized)\n",
    "    exp_scores = np.nan_to_num(np.exp(scores_norm))\n",
    "\n",
    "    # get nu (row-wise normalized)\n",
    "    nu = exp_scores.sum(axis=2)\n",
    "\n",
    "    # get eigenvalues by dividing nu=i with nu=i+1\n",
    "    eta = np.divide(nu[:,:-1,:],nu[:,1:,:])\n",
    "\n",
    "    # division of two values with different scaling/normalization requires multiplication with inverse scaling/normalization\n",
    "    scores_max_np = scores_max.detach().cpu().numpy()\n",
    "    score_max_diff = -scores_max_np[:,1:,:]+scores_max_np[:,:-1,:]\n",
    "    max_scaling = np.exp(score_max_diff.astype(np.float64))\n",
    "\n",
    "    eta = eta*max_scaling\n",
    "\n",
    "    # add dimension for concatenation\n",
    "    eta = np.expand_dims(eta, axis=-1)\n",
    "\n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142c1a8",
   "metadata": {},
   "source": [
    "#### Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "try:\n",
    "  data_dir = f\"/content/drive/MyDrive/datasets/{dataset._name_}_eigs\"\n",
    "except:\n",
    "  # resort to wikitext\n",
    "  data_dir = f\"/content/drive/MyDrive/datasets/wikitext_eigs\"\n",
    "\n",
    "print(data_dir)\n",
    "if os.path.isdir(data_dir):\n",
    "    ignore_eig_comp = -1\n",
    "\n",
    "    pattern = re.compile(r\"^eigs_(\\d+)\\.pkl$\")\n",
    "    for fname in os.listdir(data_dir):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            idx = int(match.group(1))\n",
    "            ignore_eig_comp = max(ignore_eig_comp, idx)\n",
    "    print(f\"Ignoring {ignore_eig_comp} first batches.\")\n",
    "else:\n",
    "    print(f\"{data_dir} is not a directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import dill\n",
    "\n",
    "compute_eigs = True\n",
    "\n",
    "# load metrics\n",
    "metrics_fn = dataset.get_metrics(layer=\"transformer\")\n",
    "\n",
    "bsz = 2 # 4 seems to be the max given the available ram on T4 for wikitext\n",
    "testloader = dataset.test_dataloader(batch_size=bsz, shuffle=False)\n",
    "if type(testloader) is dict:\n",
    "        testloader = testloader[None]\n",
    "\n",
    "eigs = []\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "model.eval()\n",
    "\n",
    "test_performance = 0.0\n",
    "test_loss = 0.0\n",
    "\n",
    "batch = 0\n",
    "with torch.inference_mode():\n",
    "    for X, y, _ in tqdm(testloader):\n",
    "        X = X.to(model.device)\n",
    "        y = y.to(model.device).view(-1)\n",
    "\n",
    "        output = model(X)\n",
    "\n",
    "        if compute_eigs:\n",
    "          eig = np.empty((bsz, X.shape[1] - 1, num_heads, num_layers))\n",
    "          if batch > ignore_eig_comp:\n",
    "            for layer_idx in tqdm(range(num_layers), leave=False):\n",
    "\n",
    "              Q = qkv_cache[layer_idx][\"Q\"] # (batch, seq_len, hidden_dim)\n",
    "              K = qkv_cache[layer_idx][\"K\"]\n",
    "              V = qkv_cache[layer_idx][\"V\"]\n",
    "\n",
    "              head_dim = Q.shape[-1] // num_heads\n",
    "              kv_head_dim = K.shape[-1] // num_kv_heads\n",
    "\n",
    "\n",
    "\n",
    "              # Reshape to (batch, seq_len, num_heads, head_dim)\n",
    "              Q_head = Q.view(Q.shape[0], Q.shape[1], num_heads, head_dim)\n",
    "              K_head = K.view(K.shape[0], K.shape[1], num_kv_heads, kv_head_dim)\n",
    "              V_head = V.view(V.shape[0], V.shape[1], num_kv_heads, kv_head_dim)\n",
    "\n",
    "              eig[:,:,:,layer_idx] = get_eig_from_qkv_att_softmax(Q_head, K_head, V_head).squeeze() # (batch_size, seq_len-1, num_heads, 1)\n",
    "\n",
    "            path = f\"/content/drive/MyDrive/datasets/{dataset._name_}_eigs/eigs_{batch}.pkl\"\n",
    "\n",
    "            # save eigenvalues\n",
    "            with open(path, \"wb\") as f:\n",
    "                dill.dump(eig, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(output.logits.view(-1, output.logits.size(-1)), y)\n",
    "        test_loss += loss.item()\n",
    "        test_performance += metrics_fn(output.logits.view(-1, output.logits.size(-1)), y)\n",
    "\n",
    "        del X, y, output, loss\n",
    "\n",
    "        batch += 1\n",
    "\n",
    "test_loss = test_loss/len(testloader)\n",
    "test_perf = test_performance / len(testloader)\n",
    "tqdm.write(\"Test performance: {0:.4f}\\n\".format(test_perf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf63f9",
   "metadata": {},
   "source": [
    "### Eigenvalue Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216513b4",
   "metadata": {},
   "source": [
    "#### Eigenvalue processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_eigs = True\n",
    "if process_eigs:\n",
    "  import os\n",
    "  import glob\n",
    "  import dill\n",
    "  import numpy as np\n",
    "\n",
    "  path = \"/content/drive/MyDrive/datasets/wikitext_eigs/\"\n",
    "\n",
    "  # Collect all eigs_*.pkl files (sorted for determinism)\n",
    "  files = sorted(glob.glob(os.path.join(path, \"eigs_*.pkl\")))\n",
    "\n",
    "  arrays = []\n",
    "  for f in files:\n",
    "      with open(f, \"rb\") as fh:\n",
    "          arr = dill.load(fh)\n",
    "          assert isinstance(arr, np.ndarray), f\"{f} does not contain a numpy ndarray\"\n",
    "          arrays.append(arr)\n",
    "\n",
    "  # Concatenate along batch dimension\n",
    "  all_eigs = np.concatenate(arrays, axis=0)\n",
    "\n",
    "  print(all_eigs.shape) # (b_sz, seq_len-1, heads, layers-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe696305",
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_eigs:\n",
    "  np.save(\"/content/drive/MyDrive/datasets/wikitext_eigs/all_eigs.npy\", all_eigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b5168",
   "metadata": {},
   "source": [
    "#### Eigenvalue plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f915c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eigs=True\n",
    "if plot_eigs:\n",
    "  def threshold_analysis(eig_val, thresholds):\n",
    "      \"\"\"\n",
    "      eig_val: shape (B, N, num_heads, num_layers)\n",
    "      thresholds: 1D array of threshold values\n",
    "      \"\"\"\n",
    "\n",
    "      num_layers = eig_val.shape[-1]\n",
    "      num_heads = eig_val.shape[2]\n",
    "      batch_size = eig_val.shape[0]\n",
    "\n",
    "      thresholds = thresholds.flatten()\n",
    "      num_thresholds = thresholds.shape[0]\n",
    "      percentages = np.empty([num_thresholds + 1, batch_size, num_heads, num_layers])\n",
    "\n",
    "      # Values we compare against thresholds\n",
    "      # Shape: (B, N, H, L)\n",
    "      eta = eig_val\n",
    "      count_eta_all = eta.shape[1]  # total values per head/layer\n",
    "\n",
    "      # First bin: 0 <= x <= first threshold\n",
    "      mask_low = (eta >= 0) & (eta <= thresholds[0])\n",
    "      percentages[0,:,:,:] = mask_low.sum(axis=(1)) / count_eta_all * 100\n",
    "\n",
    "      # Last bin: > last threshold\n",
    "      mask_high = eta > thresholds[-1]\n",
    "      percentages[-1,:,:,:] = mask_high.sum(axis=(1)) / count_eta_all * 100\n",
    "\n",
    "      # Middle bins: thresholds[t] <= x <= thresholds[t+1]\n",
    "      for t in range(num_thresholds-1):\n",
    "          mask_middle = (eta >= thresholds[t]) & (eta <= thresholds[t+1])\n",
    "          percentages[t+1,:,:,:] = mask_middle.sum(axis=(1)) / count_eta_all * 100\n",
    "\n",
    "      return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc5fddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_eigs:\n",
    "  import numpy as np\n",
    "\n",
    "  all_eigs = np.load(\"/content/drive/MyDrive/datasets/wikitext_eigs/all_eigs.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_eigs:\n",
    "  thresholds_radius = np.array([0.1,0.5,0.9,1.0,10,100])\n",
    "  percentage = threshold_analysis(all_eigs, thresholds_radius) # output: (num_bins, num_batch, num_heads, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_eigs:\n",
    "  percentage_per_layer = percentage.mean(axis=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_eigs:\n",
    "  import matplotlib.pyplot as plt\n",
    "  import numpy as np\n",
    "\n",
    "  num_layers_to_plot = min(5, percentage_per_layer.shape[1])\n",
    "\n",
    "  # Build bin labels\n",
    "  thresholds_radius = np.asarray(thresholds_radius)\n",
    "  bin_labels = (\n",
    "      [f\"(0, {thresholds_radius[0]:.2f})\"] +\n",
    "      [f\"({thresholds_radius[i]:.2f}, {thresholds_radius[i+1]:.2f})\"\n",
    "        for i in range(len(thresholds_radius) - 1)] +\n",
    "      [f\"({thresholds_radius[-1]:.2f}, ∞)\"]\n",
    "  )\n",
    "\n",
    "  for layer in range(num_layers_to_plot):\n",
    "      values = percentage_per_layer[:, layer]  # (bins,)\n",
    "\n",
    "      plt.figure(figsize=(8, 4))\n",
    "      plt.bar(range(len(values)), values)\n",
    "      plt.xlabel(\"Eigenvalue bins\")\n",
    "      plt.ylabel(\"Percentage (%)\")\n",
    "      plt.title(f\"Percentage of Eigenvalues in Each Bin — Layer {layer}\")\n",
    "\n",
    "      plt.xticks(range(len(bin_labels)), bin_labels, rotation=45, ha=\"right\")\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b188f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_eigs:\n",
    "  percentage_per_layer_per_head = percentage.mean(axis=(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_eigs:\n",
    "  import matplotlib.pyplot as plt\n",
    "  import numpy as np\n",
    "\n",
    "  num_layers = min(5, percentage_per_layer_per_head.shape[2])\n",
    "  num_heads = min(100, percentage_per_layer_per_head.shape[1])\n",
    "\n",
    "  # Build bin labels (same as before)\n",
    "  thresholds_radius = np.asarray(thresholds_radius)\n",
    "  bin_labels = (\n",
    "      [f\"(0, {thresholds_radius[0]:.2f})\"] +\n",
    "      [f\"({thresholds_radius[i]:.2f}, {thresholds_radius[i+1]:.2f})\"\n",
    "      for i in range(len(thresholds_radius) - 1)] +\n",
    "      [f\"({thresholds_radius[-1]:.2f}, ∞)\"]\n",
    "  )\n",
    "\n",
    "  fig, axes = plt.subplots(\n",
    "      num_layers,\n",
    "      num_heads,\n",
    "      figsize=(4 * num_heads, 3 * num_layers),\n",
    "      sharex=True,\n",
    "      sharey=True\n",
    "  )\n",
    "\n",
    "  for layer in range(num_layers):\n",
    "      for head in range(num_heads):\n",
    "          ax = axes[layer, head]\n",
    "\n",
    "          # (bins,)\n",
    "          values = percentage_per_layer_per_head[:, head, layer]\n",
    "\n",
    "          ax.bar(range(len(values)), values)\n",
    "\n",
    "          if layer == num_layers - 1:\n",
    "              ax.set_xticks(range(len(bin_labels)))\n",
    "              ax.set_xticklabels(bin_labels, rotation=45, ha=\"right\")\n",
    "          else:\n",
    "              ax.set_xticks([])\n",
    "\n",
    "          if head == 0:\n",
    "              ax.set_ylabel(f\"Layer {layer}\")\n",
    "\n",
    "          if layer == 0:\n",
    "              ax.set_title(f\"Head {head}\")\n",
    "\n",
    "  fig.suptitle(\"Percentage of Eigenvalues per Bin (Layers × Heads)\", fontsize=14)\n",
    "  fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79865593",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_eigs:\n",
    "  import numpy as np\n",
    "  import matplotlib.pyplot as plt\n",
    "  from sys import path\n",
    "  from os import getcwd\n",
    "  import wandb\n",
    "  import os\n",
    "\n",
    "\n",
    "\n",
    "  def plot_all_layers_and_spec_head_without_init(\n",
    "      data_all,\n",
    "      data_all_std,\n",
    "      models,\n",
    "      common, threshold, num_head, num_layers, seed, spec, layer_select, plot_layers\n",
    "  ):\n",
    "      \"\"\"\n",
    "      data_all shape:\n",
    "          (n_bins, n_models, n_layers)\n",
    "\n",
    "      After transposition (for plotting):\n",
    "          (n_models, n_bins)\n",
    "      \"\"\"\n",
    "\n",
    "      fig_height = len(layer_select) * 7\n",
    "      fig_width = 30\n",
    "\n",
    "      fig, axes = plt.subplots(\n",
    "          len(layer_select), 1, sharex=True, figsize=(fig_width, fig_height)\n",
    "      )\n",
    "      fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "      color_seq = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'gray']\n",
    "\n",
    "      thresholds_plot = threshold.tolist()\n",
    "      thresholds_plot.insert(0, 0.0)\n",
    "\n",
    "      for j, layer in enumerate(layer_select):\n",
    "\n",
    "          # -----------------------------------------\n",
    "          # Slice layer and FIX DIMENSIONS\n",
    "          # -----------------------------------------\n",
    "          # original: (bins, models)\n",
    "          data = data_all[:, :, layer].T\n",
    "          data_std = data_all_std[:, :, layer].T\n",
    "\n",
    "\n",
    "          # now: (models, bins)\n",
    "          n_models, n_bins = data.shape\n",
    "\n",
    "          x = np.arange(n_models)   # one group per model\n",
    "          width = 0.09\n",
    "\n",
    "          error_config = dict(\n",
    "              elinewidth=3,\n",
    "              capsize=5,\n",
    "              capthick=3,\n",
    "              zorder=4\n",
    "          )\n",
    "\n",
    "\n",
    "          yerr_lower = np.minimum(data_std, data)\n",
    "          yerr_upper = data_std\n",
    "          final_err = [yerr_lower, yerr_upper]\n",
    "\n",
    "          # -----------------------------------------\n",
    "          # Plot bins as grouped bars\n",
    "          # -----------------------------------------\n",
    "          for i in range(n_bins):\n",
    "\n",
    "              offset = i * (width + 0.04)\n",
    "\n",
    "              if i == n_bins - 1:\n",
    "                  label_tr   = f'(t) > {thresholds_plot[i]}'\n",
    "              else:\n",
    "                  label_tr   = f'(t) {thresholds_plot[i]}-{thresholds_plot[i+1]}'\n",
    "\n",
    "\n",
    "              axes[j].bar(\n",
    "                  x + offset,\n",
    "                  data[:, i],\n",
    "                  width=width,\n",
    "                  color=color_seq[i],\n",
    "                  edgecolor='black',\n",
    "                  yerr=[yerr_lower[:, i], yerr_upper[:, i]],\n",
    "                  error_kw=error_config,\n",
    "                  capsize=4,\n",
    "                  label=label_tr,\n",
    "                  zorder=3\n",
    "              )\n",
    "\n",
    "          # -----------------------------------------\n",
    "          # Axis styling\n",
    "          # -----------------------------------------\n",
    "          axes[j].tick_params(axis='x', labelsize=60)\n",
    "          axes[j].tick_params(axis='y', labelsize=60)\n",
    "          # axes[j].set_xticks(x + (width+0.04)*(n_models-1)/2, models)\n",
    "          axes[j].set_yticks([0, 25, 50, 75, 100])\n",
    "          axes[j].tick_params(axis='y', labelleft=False)\n",
    "          axes[j].grid(axis='y', zorder=1)\n",
    "          axes[j].set_ylim([0, 110])\n",
    "\n",
    "          group_width = n_bins * (width + 0.04) - 0.04\n",
    "          x_center = x + group_width / 2 - width / 2\n",
    "\n",
    "          axes[j].set_xticks(x_center)\n",
    "          axes[j].set_xticklabels(models, fontsize=45)\n",
    "          axes[j].tick_params(axis='x', length=0)\n",
    "\n",
    "          axes[j].text(\n",
    "              0.01, 0.99,\n",
    "              f'layer {plot_layers[layer] + 1}',\n",
    "              transform=axes[j].transAxes,\n",
    "              ha='left', va='top',\n",
    "              fontsize=47, fontweight='bold',\n",
    "              zorder=3,\n",
    "              bbox=dict(\n",
    "                  facecolor=\"white\",\n",
    "                  edgecolor=\"black\",\n",
    "                  boxstyle=\"round,pad=0.3\"\n",
    "              )\n",
    "          )\n",
    "\n",
    "          # axes[j].set_title(common,\n",
    "          #                         fontsize=60,\n",
    "          #                         fontweight='bold',\n",
    "          #                         pad=25)\n",
    "\n",
    "          # Vertical separators between models\n",
    "          for xi in range(1, n_models):\n",
    "              axes[j].axvline(\n",
    "                  xi - 0.13,\n",
    "                  color=\"black\",\n",
    "                  linewidth=2,\n",
    "                  linestyle=\"--\",\n",
    "                  zorder=0\n",
    "              )\n",
    "          for label in axes[j].get_xticklabels():\n",
    "              label.set_ha('center')\n",
    "\n",
    "      for ax in axes:\n",
    "          ax.margins(x=0)\n",
    "\n",
    "      # plt.suptitle(common, fontsize=67, fontweight='bold')\n",
    "      # plt.subplots_adjust(top=0.93)\n",
    "\n",
    "      # --- compute x-centers of model groups (same logic as xticks) ---\n",
    "      group_width = n_bins * (width + 0.04) - 0.04\n",
    "      x_center = x + group_width / 2 - width / 2\n",
    "\n",
    "      # convert data x-coordinates → figure coordinates\n",
    "      xlims = axes[0].get_xlim()\n",
    "      x_fig = [(xc - xlims[0]) / (xlims[1] - xlims[0]) for xc in x_center]\n",
    "\n",
    "      fig.text(\n",
    "      0.5, 0.9,\n",
    "      \"WikiText\",\n",
    "      ha=\"center\", va=\"top\",\n",
    "      fontsize=55,\n",
    "      fontweight=\"bold\"\n",
    "      )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      fig.savefig(\n",
    "          f'{common}_head{num_head}_all_layers{num_layers}_seed{seed}_without_init_{spec}.pdf',\n",
    "          format='pdf',\n",
    "          dpi=300,\n",
    "          bbox_inches='tight'\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73558a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_eigs:\n",
    "  # percentage: (n_bins, b_sz, heads, layers)\n",
    "  mean_pct = percentage.mean(axis=1)  # (n_bins, heads, layers)\n",
    "  std_pct  = percentage.std(axis=1)   # (n_bins, heads, layers)\n",
    "\n",
    "  plt.rcParams.update({'font.size': 55})\n",
    "  plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "  plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "\n",
    "  plot_heads = [0, 1, 2, 8, 9, 17]\n",
    "\n",
    "  plot_layers = [0, 1, 2, 3, 4, 20, 21]\n",
    "\n",
    "  # Select heads along the \"models\" dimension\n",
    "  data_all = mean_pct[:, plot_heads, :]      # (n_bins, n_selected_heads, layers)\n",
    "  data_all_std = std_pct[:, plot_heads, :]\n",
    "\n",
    "  data_all = data_all[:, :, plot_layers]\n",
    "  data_all_std = data_all_std[:, :, plot_layers]\n",
    "\n",
    "  n_models = len(plot_heads)\n",
    "  models = [f'Head {h}' for h in plot_heads]\n",
    "\n",
    "\n",
    "  layer_select = list(range(len(plot_layers)))\n",
    "\n",
    "  plot_all_layers_and_spec_head_without_init(\n",
    "      data_all=data_all,\n",
    "      data_all_std=data_all_std,\n",
    "      models=models,\n",
    "      common=\"Eigenvalue radius %\",\n",
    "      threshold=thresholds_radius,\n",
    "      num_head=len(plot_heads),\n",
    "      num_layers=len(plot_layers),\n",
    "      seed=0,\n",
    "      spec=\"radius\",\n",
    "      layer_select=layer_select,\n",
    "      plot_layers=plot_layers\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f0e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
