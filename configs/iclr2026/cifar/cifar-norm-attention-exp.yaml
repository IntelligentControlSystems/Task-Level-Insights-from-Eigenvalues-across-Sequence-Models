seed: 1919
save: "./checkpoint/cifar-10-norm-attention-exp"
dataset:
  name: "CIFAR-10"
  _name_: "cifar"
  grayscale: True
  tokenize: False
train:
  num_epochs: 50
  batch_size: 50
  lr: 0.0002
  wd: 0.0
  warmup: 5
  param_group: ~
model:
  # task specific dims
  input_dim: 1
  output_dim: 10
  # backbone model
  layer: "transformer"
  attention_fn: "norm-attention"
  use_flash: False
  mode: "attention"
  norm_fn: "exp"
  approx_fn: "none"
  scale_B: True
  offset: True
  offset_init: "exp"
  learn_A: False # set this to False!
  dim_conv: 4
  num_layers: 6
  hidden_dim: 512
  state_dim: 64
  num_heads: 4
  att_dropout: 0.0
  norm: "layer"
  # embedding & positional embedding
  # WARNING: make sure to set dataset.tokenize to True
  embedding: False
  vocab_size: 256 # discrete byte inputs
  max_pos_embed: 0 # 0 means no positional encoding; otherwise length of input (1024)
  # mixer
  mixer: "mlp"
  mixer_dim: 128
  # global dropout rate
  dropout: 0.0
  # classifier
  classifier: True
  pooling: "mean"
  # use dual classification
  dual: False
