seed: 1919
save: "./checkpoint/wikitext-mamba2"
dataset:
  name: "WikiText"
  _name_: "wikitext"
  version: 103
  block_size: 1024
  data_dir: ""
  fixed_size: True
train:
  total_steps: 130000
  batch_size: 8
  lr: 0.001
  wd: 0.1
  warmup_steps: 3000
  eval_every: 1000
  betas: [0.9, 0.95]
  param_group: ~
model:
  # task specific dims
  input_dim: 1
  output_dim: 50257
  layer: "mamba"
  version: "mamba2"
  num_layers: 6
  hidden_dim: 512
  state_dim: 512
  conv_dim: 4
  expansion: 1
  dropout: 0.0
  glu: True
  norm: "layer"
  dual: False
  prenorm: True
  pooling: "none"
  embedding: True
  token_embedding: True
  num_heads: 8
  norm: "layer"
  dual: False
  prenorm: True
  # embedding & positional embedding
  embedding: True
  token_embedding: True
  vocab_size: 50257 
  max_pos_embed: 1024