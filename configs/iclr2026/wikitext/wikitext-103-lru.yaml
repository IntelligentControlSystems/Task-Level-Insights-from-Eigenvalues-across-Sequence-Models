seed: 1919
save: "./checkpoint/wikitext-lru"
dataset:
  name: "WikiText"
  _name_: "wikitext"
  version: 103
  block_size: 1024
  data_dir: ""
  fixed_size: True
train:
  total_steps: 130000
  batch_size: 8
  lr: 0.001
  wd: 0.1
  ssm_lr: 0.001
  lr_min: 0.0000001
  reduce_factor: 0.5
  lr_patience: 5
  warmup_steps: 13000
  eval_every: 1000
  betas: [0.9, 0.95]
  param_group: ~
  cosine_anneal: True
model:
  layer: "lru"
  dt_min: 0.001
  dt_max: 0.1
  num_layers: 6
  activation: "full_glu"
  input_dim: 50257
  output_dim: 50257
  hidden_dim: 512
  state_dim: 512
  dropout: 0
  norm: "batch"
  pooling: "none"
  ssm_lr_vars: [ "Lambda_re", "Lambda_im", "P", "B", "log_step" ]
  prenorm: False
  dual: False
  decode: False
optimization:
  jax_seed: 1919

