seed: 1919
save: "./checkpoint/imdb-sm-attention"
dataset:
  name: "IMDB"
  _name_: "imdb"
  fixed_size: False
  data_dir: ""
train:
  num_epochs: 30
  batch_size: 6
  lr: 0.0002
  wd: 0.1
  warmup: 5
  param_group: ~
model:
  # task specific dims
  input_dim: 1
  output_dim: 2
  # backbone model
  layer: "transformer"
  attention_fn: "sm-attention"
  num_layers: 4
  hidden_dim: 128
  state_dim: 64 
  num_heads: 4
  att_dropout: 0.1
  norm: "layer"
  # embedding & positional embedding
  embedding: True
  vocab_size: 134 
  max_pos_embed: 4096
  # mlp
  mixer: "mlp"
  mixer_dim: 512
  # global dropout rate
  dropout: 0.1
  # classifier
  classifier: True
  pooling: "mean"
  # use dual classification
  dual: False
